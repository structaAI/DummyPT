# DummyPT-124M-Base (124 Million Parameters)
model:
  name: "dummypt-124m-base"
  version: "1.0"
  total_params: 124000000
  
  # Core Architecture
  d_model: 768                # Hidden dimension
  n_layers: 12                # Transformer layers
  n_heads: 12                 # Attention heads
  head_dim: 64                # 768/12 = 64
  
  # Feed-Forward Network
  ff_dim: 3072                # 4 * d_model
  ffn_multiplier: 4
  activation: "gelu"
  gated_ffn: false
  
  # Embeddings
  vocab_size: 50257           # GPT-2 vocabulary
  max_position_embeddings: 2048
  rope_theta: 10000.0
  rope_scaling: null
  
  # Normalization & Dropout
  layer_norm_eps: 1e-5
  rms_norm: true
  attention_dropout: 0.1
  residual_dropout: 0.1
  embedding_dropout: 0.1
  
  # Bias Configuration
  bias: false                 # Llama-style (no bias)
  use_qkv_bias: false
  
  # Initialization
  initializer_range: 0.02
  rescale_layers: 1.0
  
  # Memory Estimates
  memory_fp32: "496 MB"       # 124M * 4 bytes
  memory_fp16: "248 MB"
  memory_int8: "124 MB"
  memory_int4: "62 MB"

training:
  global_batch_size: 128
  micro_batch_size: 32
  gradient_accumulation_steps: 4
  
  # Learning Rate
  learning_rate: 3e-4
  min_lr: 1e-5
  warmup_steps: 1000
  decay_steps: 50000
  
  # Optimizer
  optimizer: "adamw"
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  
  # Mixed Precision
  fp16: true
  bf16: false
  fp16_opt_level: "O2"
  
  # Memory Optimizations
  gradient_checkpointing: false  # Not needed for 124M
  offload_optimizer: false
  offload_param: false

inference:
  max_new_tokens: 1024
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  do_sample: true
  
  # Performance
  use_cache: true
  use_flash_attention: true
  prefill_chunk_size: 512
  
  # Hardware Requirements
  vram_fp16: "500 MB"
  vram_int8: "250 MB"
  cpu_ram_fp32: "1.5 GB"
  cpu_ram_int8: "750 MB"